{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exp3_DS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Sakshi Patil **\n",
        "<br> B 2018130039\n",
        "\n",
        "##Experiment 3\n",
        "\n",
        "**Objective:**\n",
        "\n",
        "(i) Apply Naive bayes classifier for given data see Naive_bayes_handout.zip file in edmodo.\n",
        "\n",
        "(ii) Apply and compare Linear Discriminating Analysis with Naive bayes classifiers w.r.t. error, specificity and sensitivity"
      ],
      "metadata": {
        "id": "N58KwdZef0I9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the construct an python code for:\n",
        "\n",
        "1. Convert the textual meta-data into a suitable (e.g. corpus) object.\n",
        "\n",
        "2. Triage some of the irrelevant punctuation and other symbols in the corpus document,change all text to lower case, etc.\n",
        "\n",
        "3. Tokenize the job descriptions into words. Examine the distributions of two important features\n",
        "\n",
        "4. Classify attributes in two categories.\n",
        "\n",
        "5. Graphically visualize the difference between low and high value of targeted feature graph.\n",
        "\n",
        "6. Transform the features into categorical data\n",
        "\n",
        "7. Ignore those low frequency words and report the sparsity of your categorical data matrix with or without delete those low frequency words. Note that the sparsity of a matrix is the fraction:\n",
        "\n",
        "Sparsity(A) = number of zero-valued elements / total number of matrix elements (m×n).\n",
        "\n",
        "1. Apply the Naive Bayes classifier to original matrix and lower dimension matrix, what do you observe?\n",
        "\n",
        "2. Apply and compare LDA and Naive Bayes classifiers with respect to the error, specificity and sensitivity."
      ],
      "metadata": {
        "id": "LijqgbpS9agO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "4qh1Qmeka_2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!pip install scikit-plot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSui5CNC9dgt",
        "outputId": "c753e8f8-2449-4b9c-fe81-72f6b4dc2271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Collecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.0.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.21.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->scikit-plot) (3.1.0)\n",
            "Installing collected packages: scikit-plot\n",
            "Successfully installed scikit-plot-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/DatasetS/IMDB/IMDB Dataset.csv.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loDmefvOhnGc",
        "outputId": "383704e9-daed-406e-ecd8-970a01bf65fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/DatasetS/IMDB/IMDB Dataset.csv.zip, /content/drive/MyDrive/DatasetS/IMDB/IMDB Dataset.csv.zip.zip or /content/drive/MyDrive/DatasetS/IMDB/IMDB Dataset.csv.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1bA7Ayofw_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7869f67-4d11-4782-f60f-43192d313519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scikitplot as skplt\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "#from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker \n",
        "from spellchecker import SpellChecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY2TF6WXdBUa",
        "outputId": "8af03b8f-a486-4bf9-e97e-a002910f330e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.6.3-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 7.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df = pd.read_csv('/content/Corona_NLP_train.csv', encoding='latin-1')\n",
        "imdb_data = pd.read_csv(\"/content/IMDB Dataset.csv\")\n",
        "\n",
        "tweets_df = tweets_df[['OriginalTweet', 'Sentiment']]\n",
        "\n",
        "test_df = pd.read_csv('/content/Corona_NLP_test.csv')\n",
        "test_df = test_df[['OriginalTweet', 'Sentiment']]"
      ],
      "metadata": {
        "id": "b54jze8m9doz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values\n",
        "for column in tweets_df.columns:\n",
        "    print(tweets_df[column].isnull().value_counts())\n",
        "print()  \n",
        "for column in test_df.columns:\n",
        "    print(test_df[column].isnull().value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSuJk7NHb_6q",
        "outputId": "f444a4ff-6fe4-41d6-897c-9c98d39b994b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False    41157\n",
            "Name: OriginalTweet, dtype: int64\n",
            "False    41157\n",
            "Name: Sentiment, dtype: int64\n",
            "\n",
            "False    3798\n",
            "Name: OriginalTweet, dtype: int64\n",
            "False    3798\n",
            "Name: Sentiment, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate values\n",
        "print(tweets_df.duplicated().value_counts())\n",
        "print(test_df.duplicated().value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySR8XQBOb_97",
        "outputId": "92536dfa-2b2b-44f4-9c4c-b443704f2dfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False    41157\n",
            "dtype: int64\n",
            "False    3798\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "#ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "spell = SpellChecker()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-1LNmeNcABr",
        "outputId": "4fa22aab-5ca1-4f00-a540-0fc5742dd3de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "puncs_ = string.punctuation.replace('@','')\n",
        "puncs = puncs_.replace('#','')\n",
        "puncs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_mu1bO3zcAOp",
        "outputId": "c1669556-7c73-4d58-b592-1c1ca79825a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def textClean(text):\n",
        "    # convert to lowercase\n",
        "    lower = [char.lower() for char in text if char not in puncs]\n",
        "    lower = ''.join(lower)\n",
        "    lower = ' '.join(lower.split())\n",
        "    \n",
        "    # delete @mentions and #tags\n",
        "    for char in lower:\n",
        "        if lower.find('@')==-1 and lower.find('#')==-1: # break loop once @ and # is over\n",
        "            break\n",
        "        if (char=='@' or char=='#'):\n",
        "            try:\n",
        "                char_index = lower.index(char)\n",
        "            except ValueError:\n",
        "                #print(lower)\n",
        "                break\n",
        "            del_word = ''\n",
        "            while char not in string.whitespace:\n",
        "                del_word = del_word+lower[char_index]\n",
        "                char_index = char_index + 1\n",
        "                try:\n",
        "                    char = lower[char_index] #trying for indexerror incase it is the last character of string\n",
        "                except IndexError:\n",
        "                    char = ' '\n",
        "                except:\n",
        "                    print(\"Something else went wrong\")\n",
        "            lower = lower.replace(del_word,'',1)\n",
        "    lower = [char for char in lower if char not in string.punctuation and char not in string.digits]\n",
        "    lower = ''.join(lower)\n",
        "    \n",
        "    tokens = word_tokenize(lower)\n",
        "    nohttp = [word for word in tokens if word[0:4]!='http']\n",
        "    nostop = [word for word in nohttp if word not in stopwords.words('english')]\n",
        "    return nostop"
      ],
      "metadata": {
        "id": "mOSyGiyAdO4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_list = tweets_df.OriginalTweet[0:10].apply(textClean)\n",
        "for each_list in temp_list:\n",
        "    print(each_list)#for word in each_list:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tkJPcrsdPOO",
        "outputId": "696d221a-1d01-4292-fb38-33aa7122eac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "['advice', 'talk', 'neighbours', 'family', 'exchange', 'phone', 'numbers', 'create', 'contact', 'list', 'phone', 'numbers', 'neighbours', 'schools', 'employer', 'chemist', 'gp', 'set', 'online', 'shopping', 'accounts', 'poss', 'adequate', 'supplies', 'regular', 'meds', 'order']\n",
            "['coronavirus', 'australia', 'woolworths', 'give', 'elderly', 'disabled', 'dedicated', 'shopping', 'hours', 'amid', 'covid', 'outbreak']\n",
            "['food', 'stock', 'one', 'empty', 'please', 'dont', 'panic', 'enough', 'food', 'everyone', 'take', 'need', 'stay', 'calm', 'stay', 'safe']\n",
            "['ready', 'go', 'supermarket', 'outbreak', 'im', 'paranoid', 'food', 'stock', 'litteraly', 'empty', 'serious', 'thing', 'please', 'dont', 'panic', 'causes', 'shortage']\n",
            "['news', 'regionâ\\x92s', 'first', 'confirmed', 'covid', 'case', 'came', 'sullivan', 'county', 'last', 'week', 'people', 'flocked', 'area', 'stores', 'purchase', 'cleaning', 'supplies', 'hand', 'sanitizer', 'food', 'toilet', 'paper', 'goods', 'reports']\n",
            "['cashier', 'grocery', 'store', 'sharing', 'insights', 'prove', 'credibility', 'commented', 'im', 'civics', 'class', 'know', 'im', 'talking']\n",
            "['supermarket', 'today', 'didnt', 'buy', 'toilet', 'paper']\n",
            "['due', 'covid', 'retail', 'store', 'classroom', 'atlanta', 'open', 'walkin', 'business', 'classes', 'next', 'two', 'weeks', 'beginning', 'monday', 'march', 'continue', 'process', 'online', 'phone', 'orders', 'normal', 'thank', 'understanding']\n",
            "['corona', 'preventionwe', 'stop', 'buy', 'things', 'cash', 'use', 'online', 'payment', 'methods', 'corona', 'spread', 'notes', 'also', 'prefer', 'online', 'shopping', 'home', 'time', 'fight', 'covid']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(analyzer=textClean)\n",
        "message = vectorizer.fit_transform(tweets_df['OriginalTweet'])\n",
        "message.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6PGclnceHDi",
        "outputId": "9e90af07-e98e-4429-8d3b-59bdaac8bb0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41157, 39097)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data into 80% training and 20% testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(message,tweets_df.Sentiment,test_size=0.20,random_state=123)"
      ],
      "metadata": {
        "id": "g_H7sfbzeQ6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create and train the Naive Bayes Classifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB().fit(xtrain, ytrain)"
      ],
      "metadata": {
        "id": "OaWoQ99xeQ87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classifier.predict(xtest))\n",
        "print(ytest.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oKU5l4xeHOV",
        "outputId": "015e1a5d-9c51-4977-acdf-5d35e450073e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Negative' 'Positive' 'Positive' ... 'Neutral' 'Extremely Negative'\n",
            " 'Positive']\n",
            "['Negative' 'Positive' 'Neutral' ... 'Neutral' 'Extremely Negative'\n",
            " 'Positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the training data set\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "pred = classifier.predict(xtrain)\n",
        "print(classification_report(ytrain, pred))\n",
        "print()\n",
        "print(\"Confusion Matrix: \\n\", confusion_matrix(ytrain, pred))\n",
        "print(\"Accuracy: \\n\", accuracy_score(ytrain, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay3qVDtgdPTS",
        "outputId": "313091b0-f9d3-4d07-bd6a-6c952ab1ca73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Extremely Negative       0.89      0.65      0.75      4360\n",
            "Extremely Positive       0.83      0.71      0.77      5273\n",
            "          Negative       0.69      0.79      0.74      7970\n",
            "           Neutral       0.93      0.55      0.69      6206\n",
            "          Positive       0.63      0.86      0.73      9116\n",
            "\n",
            "          accuracy                           0.73     32925\n",
            "         macro avg       0.79      0.71      0.73     32925\n",
            "      weighted avg       0.77      0.73      0.73     32925\n",
            "\n",
            "\n",
            "Confusion Matrix: \n",
            " [[2823   28 1084   39  386]\n",
            " [  17 3761  196   16 1283]\n",
            " [ 221  145 6304   91 1209]\n",
            " [  51  138  917 3383 1717]\n",
            " [  71  479  643   92 7831]]\n",
            "Accuracy: \n",
            " 0.7320273348519363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the testing data set\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "pred = classifier.predict(xtest)\n",
        "print(classification_report(ytest, pred))\n",
        "print()\n",
        "print(\"Confusion Matrix: \\n\", confusion_matrix(ytest, pred))\n",
        "print(\"Accuracy: \\n\", accuracy_score(ytest, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bkl-daUIeh7w",
        "outputId": "8d08ace4-15ef-4800-fe4e-45f586a267ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Extremely Negative       0.62      0.38      0.47      1121\n",
            "Extremely Positive       0.60      0.44      0.51      1351\n",
            "          Negative       0.42      0.51      0.46      1947\n",
            "           Neutral       0.65      0.32      0.43      1507\n",
            "          Positive       0.42      0.63      0.50      2306\n",
            "\n",
            "          accuracy                           0.48      8232\n",
            "         macro avg       0.54      0.45      0.47      8232\n",
            "      weighted avg       0.52      0.48      0.48      8232\n",
            "\n",
            "\n",
            "Confusion Matrix: \n",
            " [[ 427    9  536   20  129]\n",
            " [  10  591   77   24  649]\n",
            " [ 192   63  989   99  604]\n",
            " [  25   45  336  479  622]\n",
            " [  33  274  432  115 1452]]\n",
            "Accuracy: \n",
            " 0.478377065111759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion:"
      ],
      "metadata": {
        "id": "RctsJEdB9d2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Through this experiment i performed naive bayes classifier and it's comparison with linear discriminating analysis wrt sensitivity and specificity\n",
        "The comparison of LDA and Naive Bayes based on various metrics such as Specificity and Sensitivity was performed. The values for this metrics were observed to be increased after reducing the dimensions.\n",
        "Dimensionality Reduction reduces the time and storage space required. It helps remove multi-collinearity which improves the interpretation of the parameters of the machine learning model. It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D."
      ],
      "metadata": {
        "id": "rhbI0esyKbWS"
      }
    }
  ]
}