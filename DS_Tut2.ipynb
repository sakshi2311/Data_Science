{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " DS_Tut2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sakshi Patil\n",
        "\n",
        "BE COMPS\n",
        "\n",
        "B 2018130039"
      ],
      "metadata": {
        "id": "_75trKYsevhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tut 2:** Feature Engineering: Dimensional Reduction"
      ],
      "metadata": {
        "id": "Iy9ismyFQIRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Questions:<br>\n",
        "####**Q1. What is ‘Curse of Dimensionality’? Explain**<br>\n",
        "\n",
        "Curse of dimensionality was first mentioned in the book \"Dynamic Programming\" by mathmacian R. Bellman. It means that the error increases as the number of features increase. This is pertinent to the complexity of algorithms needed to tackle high dimensionality as well as the exponential increase in running time. Even though higher dimansionality means more information it also means possibility of more noise and redundancy. The difficulty in analysing high-dimensional data results from the conjunction of two effects.\n",
        "\n",
        "High-dimensional spaces have geometrical properties which are counter-intuitive, and far from the properties which can be observed in two-or three-dimensional spaces.\n",
        "Data analysis tools are most often designed having in mind intuitive properties and examples in low-dimensional spaces and usually, data analysis tools are best illustrated in 2-or 3-dimensional spaces.\n",
        "The difficulty is that those tools are also used when data are high-dimensional and more complex and hence, there is a probability to lose the intuition of the behavior of the tool and thus drawing incorrect conclusions.\n",
        "\n",
        "####**Q2. What is feature selection? Why is it needed? What are the different approaches of feature selection?**<br>\n",
        "Feature selection means selecting only the features that are not redundant, relevant and consistent. Using proper methods of selection we can improve performance while reducing the running time.\n",
        "\n",
        "The main benefits of performing feature selection in advance, rather than letting the machine learning model figure out which features are most important, include:\n",
        "\n",
        "1. **Simpler models:** simple models are easy to explain - a model that is too complex and unexplainable is not valuable\n",
        "2. **Shorter training times:** a more precise subset of features decreases the amount of time needed to train a model\n",
        "3. **Variance reduction:** increase the precision of the estimates that can be obtained for a given simulation \n",
        "4. **Avoid the curse of high dimensionality:** dimensionally cursed phenomena states that, as dimensionality and the number of features increases, the volume of space increases so fast that the available data become limited - PCA feature selection may be used to reduce dimensionality \n",
        "\n",
        "Methods:\n",
        "1. **Filter methods:** This uses statistics to get the features to be selected instead of cross validation. A fixed metric is used to choose features recursively.\n",
        "2. **Wrapper methods:** After selecting a set of features they are evaluated and evaluating various combinations we decide which is best. This allows to see the interaction of the features.\n",
        "3. **Embedded methods:** Embedded feature selection methods integrate the feature selection machine learning algorithm as part of the learning algorithm, in which classification and feature selection are performed simultaneously.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eY_EGnb7QnNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Problem 1<br>\n",
        "<p>\n",
        "Linear Discriminant Analysis, or LDA for short, is a classification machine learning algorithm.\n",
        "\n",
        "It works by calculating summary statistics for the input features by class label, such as the mean and standard deviation. These statistics represent the model learned from the training data. In practice, linear algebra operations are used to calculate the required quantities efficiently via matrix decomposition.\n",
        "\n",
        "Predictions are made by estimating the probability that a new example belongs to each class label based on the values of each input feature. The class that results in the largest probability is then assigned to the example.\n",
        "\n",
        "LDA assumes that the input variables are numeric and normally distributed and that they have the same variance (spread). If this is not the case, it may be desirable to transform the data to have a Gaussian distribution and standardize or normalize the data prior to modeling.\n",
        "\n",
        "The LDA model is naturally multi-class. This means that it supports two-class classification problems and extends to more than two classes (multi-class classification) without modification or augmentation.\n",
        "\n",
        "It is a linear classification algorithm, like logistic regression. This means that classes are separated in the feature space by lines or hyperplanes. Extensions of the method can be used that allow other shapes, like Quadratic Discriminant Analysis (QDA), which allows curved shapes in the decision boundary.\n",
        "\n",
        "We can use PCA to calculate a projection of a dataset and select a number of dimensions or principal components of the projection to use as input to a model.\n",
        "\n",
        "The scikit-learn library provides the PCA class that can be fit on a dataset and used to transform a training dataset and any additional dataset in the future.\n",
        "\n",
        "The Linear Discriminant Analysis is available in the scikit-learn Python machine learning library via the LinearDiscriminantAnalysis class.\n",
        "</p>"
      ],
      "metadata": {
        "id": "doYU5BlVRCKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Demonstrate the Linear Discriminant Analysis method with a worked example.\n",
        "<p>\n",
        "First, define a synthetic classification dataset or any online dataset you preferred\n",
        "\n",
        "Note in following information simply based on sytheitic data to explain you the steps of LDA application.\n",
        "\n",
        "Use the make_classification() function to create a dataset with 1,000 examples, each with 10 input variables.\n",
        "\n",
        "Following example creates and summarizes the dataset\n",
        "</p>"
      ],
      "metadata": {
        "id": "9pe0dSTpRRpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate a LDA model on the dataset"
      ],
      "metadata": {
        "id": "CZEevEQhOXOF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HNN5Ih5OAp-"
      },
      "outputs": [],
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define Dataset"
      ],
      "metadata": {
        "id": "le2GBez2Of9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=3)"
      ],
      "metadata": {
        "id": "NFsKLaIrOKmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEfI5COkOpt5",
        "outputId": "45a0df09-3f89-448c-de22-c6bd61034226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 10) (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define Model"
      ],
      "metadata": {
        "id": "dqlcVD5VOkWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearDiscriminantAnalysis()"
      ],
      "metadata": {
        "id": "xeUStR8FOi82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define model evaluation method"
      ],
      "metadata": {
        "id": "sMQE3oQmOuBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)"
      ],
      "metadata": {
        "id": "3SWujm9lOnGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluate model\n",
        "<p>\n",
        "Fit and evaluate a Linear Discriminant Analysis model. You can fit and evaluate a Linear Discriminant Analysis model using repeated stratified k-fold cross-validation via the RepeatedStratifiedKFold class. You will use suitable folds (e.g. 5 or more) and repeat.</p>"
      ],
      "metadata": {
        "id": "w6hH1J3VO0Ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)"
      ],
      "metadata": {
        "id": "JYc5polfOzuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Summarize result"
      ],
      "metadata": {
        "id": "3ghljxNQO4q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCRtpkwQOxyO",
        "outputId": "245bbe01-4cb5-48a5-cd2e-1a67904c52fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.833 (0.031)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running above the example evaluates the Linear Discriminant Analysis algorithm on the synthetic dataset (that I have considered) and reports the average accuracy across the three repeats of 10-fold cross-validation.\n",
        "\n",
        "I ran the model a few times to see the results."
      ],
      "metadata": {
        "id": "SUvuHRBuj-xz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Recording results<br>\n",
        "####<u>Run 1</u>\n",
        "Mean Accuracy: 0.893 (std: 0.033)\n",
        "####<u>Run 2</u>\n",
        "Mean Accuracy: 0.829 (std: 0.031)\n",
        "####<u>Run 3</u>\n",
        "Mean Accuracy: 0.833 (std: 0.031)\n"
      ],
      "metadata": {
        "id": "9P-Z5eDePU_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "s8s0BxbekMJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIiwzktJkh7z",
        "outputId": "2a49ead7-7099-48db-9018-ea6ac7b7d76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearDiscriminantAnalysis()"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqtKKnJ9kWRW",
        "outputId": "76d64b8d-f805-4f12-9c60-db921d60460c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8253731343283582"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IawyWVK0kfaB",
        "outputId": "36e1ce5a-73df-4ce6-9190-1da33ec9501d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8606060606060606"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion:\n",
        "On running LDA muliple times the accuracy changes slightly. We can see that the accuracy is fairly high for test set."
      ],
      "metadata": {
        "id": "Gv4bJw89ktWe"
      }
    }
  ]
}